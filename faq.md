# Frequently Asked Questions

1. **What do I need to start?** - To run an LLM chatbot locally you need a User Interface (SillyTavern or Oobabooga text generation webui) + a Model (LLM), that you can install with this UI.

2. **Do I need GPU?** - You do not need GPU to run LLMs. You can run them on CPU + Equivalent RAM. GPU makes models run faster.

3. **GGUF vs GGML?** - GGUF is positioned as an upgrade to GGML, offering more flexibility, extensibility, and compatibility. It aims to simplify the user experience and accommodate various models beyond llama. cpp. GGML, while a valuable early effort, had limitations that GGUF seeks to overcome.
